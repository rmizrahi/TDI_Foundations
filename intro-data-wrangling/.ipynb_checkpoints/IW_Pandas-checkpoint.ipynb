{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Pandas\n",
    "<!-- requirement: images/Data_Frame_Data_Series.png -->\n",
    "<!-- requirement: small_data/fha_by_tract.csv -->\n",
    "<!-- requirement: small_data/2013_Gaz_tracts_national.tsv -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Pandas is Python's answer to R.  It's a good tool for small(ish) data analysis -- i.e. when everything fits into memory.\n",
    "\n",
    "The basic new \"noun\" in pandas is the **DataFrame**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Nouns (objects) in Pandas\n",
    "\n",
    "### Data Frames\n",
    "\n",
    "Like a table, with rows and columns (e.g. as in SQL).  Except:\n",
    "  - The rows can be indexed by something interesting (there is special support for labels like categorical and timeseries data).  This is especially useful when you have timeseries data with potentially missing data points.\n",
    "  - Cells can store Python objects. Like in SQL, columns are type homogeneous.\n",
    "  - Instead of \"NULL\", the name for a non-existent value is \"NA\".  Unlike R, Python's data frames only support NAs in columns of some data types (basically: floating point numbers and 'objects') -- but this is mostly a non-issue (because it will \"up-cast\" integers to float64, etc.)\n",
    "  \n",
    "### Data Series:\n",
    "These are named columns of a DataFrame (more correctly, a DataFrame is a dictionary of Series).  The entries of the series have homogeneous type.\n",
    "\n",
    "![Data Frame Data Series](images/Data_Frame_Data_Series.png)\n",
    "[comment]: https://docs.google.com/drawings/d/1eQOWaG37cH6Uo13zfRVPWsA77F0U4WVG2y06Z3NOtAU/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# a data frame\n",
    "df1 = pd.DataFrame({\n",
    "    'number': [1, 2, 3],\n",
    "    'animal': ['cat', 'dog', 'mouse']\n",
    "})\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df1['animal']  # a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# the same data frame\n",
    "df2 = pd.DataFrame([\n",
    "    ('cat', 1),\n",
    "    ('dog', 2),\n",
    "    ('mouse', 3),\n",
    "], columns=['animal', 'number'])\n",
    "\n",
    "np.all(df1 == df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Verbs (operations) in Pandas\n",
    "  \n",
    "Pandas provides a \"batteries-included\" basic data analysis:\n",
    "  - **Loading data:** `read_csv`, `read_table`, `read_sql`, and `read_html`\n",
    "  - **Selection, filtering, and aggregation** (i.e. SQL-type operations): There's a special syntax for `SELECT`ing.  There's the `merge` method for `JOIN`ing.  There's also an easy syntax for what in SQL is a mouthful: Creating a new column whose value is computed from another column -- with the bonus that now the computations can use the full power of Python (though it might be faster if it didn't).\n",
    "  - **\"Pivot table\" style aggregation:** If you're an Excel cognoscenti, you may appreciate this.\n",
    "  - **NA handling:** Like R's data frames, there is good support for transforming NA values with default values / averaging tricks / etc.\n",
    "  - **Basic statistics:** e.g. `mean`, `median`, `max`, `min`, and the convenient `describe`.\n",
    "  - **Plugging into more advanced analytics:** Okay, this isn't batteries included.  But still, it plays reasonably with `sklearn`.\n",
    "  - **Visualization:** For instance `plot` and `hist`.\n",
    "  \n",
    "We'll go through a little on all of these in the context of an example.  To go through it, you must have the (output) data files from the HMDA \"Project structure\" example.  We're going to explore a dataset of mortgage insurance issued by the Federal Housing Authority (FHA).  The data is broken down by census tract and tells us how big of a player the FHA is in each tract (how many homes etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Loading data (and basic statistics / visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names =[\"State_Code\", \"County_Code\", \"Census_Tract_Number\", \"NUM_ALL\",\n",
    "        \"NUM_FHA\", \"PCT_NUM_FHA\", \"AMT_ALL\", \"AMT_FHA\", \"PCT_AMT_FHA\"]\n",
    "# Loading a CSV file, without a header (so we have to provide field names)\n",
    "df = pd.read_csv('small_data/fha_by_tract.csv', names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['GEOID'] = (df['Census_Tract_Number']*100 + 10**6 * df['County_Code']\n",
    "               + 10**9 * df['State_Code'])   # A computed field!\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To drop a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.drop('GEOID', axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Most operations produce copies (unless `inplace=True` is specified).  The `df` object still has the GEOID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'GEOID' in df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Rows can also be dropped.  Note that the indices do not reset.  The index is associated with the row, not with the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.drop(0, axis=0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "By default, rows are indexed by their position.  However, any column can be made into an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.set_index('State_Code').head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Multiple levels of indexing is possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.set_index(['State_Code', 'County_Code']).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "An index can be turned back into a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.set_index('State_Code').reset_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print \"Percentage of mortages in each census tract insured by FHA\"\n",
    "df['PCT_AMT_FHA'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also make a histogram of a column to explore the data further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['PCT_AMT_FHA'].hist(bins=50, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['AMT_ALL'].hist(bins=50, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The above distribution looks skewed, so let's look at its logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['LOG_AMT_ALL'] = np.log1p(df['AMT_ALL'])  # Create a new column to examine\n",
    "print df['LOG_AMT_ALL'].describe()\n",
    "\n",
    "df['LOG_AMT_ALL'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Indexing data frames\n",
    "\n",
    "Indexing by a column name yields a data series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['State_Code'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Indexing by a list of column names gives another data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df[['State_Code', 'County_Code']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question:** What will this return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "type(df[['State_Code']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A data frame is an iterator that yields the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[col for col in df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To index a particular element of the frame, use the `.loc` attribute.  It takes index and column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.loc[3, 'State_Code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Both can be sliced.  Unusually for Python, both endpoints are included in the slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.loc[0:3, 'State_Code':'Census_Tract_Number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Position-based indexing is available in the `.iloc` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.iloc[3, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The usual slicing convention is used for `.iloc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.iloc[0:3, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Indices in Pandas\n",
    "\n",
    "Pandas indices allow us to handle data naturally.  Elements are associated based on their index, not their order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s1 = pd.Series([1,2,3], index=['a', 'b', 'c'])\n",
    "s2 = pd.Series([3,2,1], index=['c', 'b', 'a'])\n",
    "s1 + s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s3 = pd.Series([3,2,1], index=['c', 'd', 'e'])\n",
    "s1 + s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Missing values get a NaN, but this can be replaced by a fill value of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s1.add(s3, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Function application and mapping\n",
    "\n",
    "For element-wise function application, the most straightforward thing to do is to apply numpy functions to these objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.arange(24).reshape(4,6))\n",
    "np.sin(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This relies on numpy functions automatically broadcasting themselves to work element-wise.  To apply a pure-python function to each element, use the `.applymap()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df1.applymap(lambda x: \"%.2f\" % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "However, sometimes you want to compute things column-wise or row-wise.  In this case, you will need to use the `apply` method.  For example, the following takes the range of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df1.apply(lambda x: x.max() - x.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And this takes the range of reach row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df1.apply(lambda x: x.max() - x.min(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Filtering data\n",
    "\n",
    "Now the `df[...]` notation is very flexible:\n",
    "  - It accepts column names (strings and lists of strings);\n",
    "  - It accepts column numbers (so long as there is no ambiguity with column names);\n",
    "  - It accepts _binary data series!_\n",
    "  \n",
    "This means that you can write\n",
    "\n",
    "        df[ (df['column_name2']==MD) & ( (df['column_name1']==5) | (df['column_name1']==6) ) ]\n",
    "   \n",
    "for what you would write in SQL as\n",
    "\n",
    ">         SELECT * FROM df WHERE\n",
    "            column_name2='MD\" AND (column_name1=5 OR column_name1=6)\n",
    "            \n",
    "Boolean operators on a data frame return a data series of bools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "(df['State_Code'] == 39).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "((df['State_Code'] == 39) & (df['Census_Tract_Number'] == 9613)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df[(df['State_Code'] == 39) & (df['Census_Tract_Number'] == 9613)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note:** selecting rows by binary data series only if they share the same data index!\n",
    "\n",
    "**Exercise:**\n",
    "1. Plot the histogram of percentages for different states in the same graph to compare them.\n",
    "2. Notice that there is a spike at 100%.  This means that the FHA has insured 100% of the houses in that census tract.  See what happens to the histogram when we restrict it to the case where the total number of loans is non-negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Joining data\n",
    "\n",
    "The analogue of a\n",
    "\n",
    ">             \n",
    "    SELECT * \n",
    "        FROM df1\n",
    "        INNER JOIN df2 \n",
    "        ON df1.field_name=df2.field_name;\n",
    "\n",
    "is\n",
    "\n",
    "    df_joined = df1.merge(df2, on='field_name')\n",
    "\n",
    "You can also do left / right / outer joins, mix-and-match column names, etc.  For that consult the Pandas documentation. (The example below will do a left join.)\n",
    "\n",
    "Of course, just looking at the distribution of insurance by census tract isn't interesting unless we know more about the census tract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The first row is the column names, so we don't have to specify those\n",
    "df_geo = pd.read_csv('small_data/2013_Gaz_tracts_national.tsv', sep='\\t')\n",
    "df_geo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_joined = df.merge(df_geo, on='GEOID', how='left')\n",
    "df_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Aggregating data\n",
    "\n",
    "The analog of SQL's `GROUP BY` is\n",
    "\n",
    "    grouped = df.groupby(['field_name1', ...])...\n",
    "\n",
    "The above is analogous to\n",
    ">             \n",
    "    SELECT mean(df.value1), std(df.value2) \n",
    "        FROM df\n",
    "        GROUP BY df.field_name1, ...\n",
    "\n",
    "Pandas is somewhat more flexible in how you can use grouping, not requiring you to specify an aggregation function up front.  The `.groupby()` method that can later be aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "usps_groups = df_joined.groupby('USPS')\n",
    "usps_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The reason Pandas doesn't require you to specify an aggregation function up front is because the groupby method by itself does little work. It returns a `DataFrameGroupBy` datatype that contains a dictionary of group keys to lists of row numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print type(usps_groups.groups)\n",
    "usps_groups.groups['AK'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can retrieve the group of data associated with one key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "usps_groups.get_group('AK').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Observe that this is the same as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_joined.iloc[usps_groups.groups['AK'][:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "usps_groups.mean().head()  # Takes the mean of the rows in each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This is the analog of\n",
    "# SELECT USPS, SUM(AMT_FHA), SUM(AMT_ALL), ... FROM df GROUP BY USPS;\n",
    "df_by_state = usps_groups['AMT_FHA', 'AMT_ALL', 'NUM_FHA', 'NUM_ALL'].sum()\n",
    "df_by_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_by_state['PCT_AMT_FHA'] = 100.0 * df_by_state['AMT_FHA']  / df_by_state['AMT_ALL']\n",
    "\n",
    "# This sure looks different than the census-tract level histogram!\n",
    "df_by_state['PCT_AMT_FHA'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can also specify a specific aggregation function per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "usps_groups['NUM_FHA', 'NUM_ALL'].agg({'NUM_FHA': np.sum,\n",
    "                                       'NUM_ALL': np.mean}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The groupby function is especially useful when you define your own aggregation functions. Here, we define a function that returns the row for the census track located farthest to the north. The apply function attempts to 'combine results together in an intelligent way.' The list of Series objects from each call to `farthest_north` for each USPS code is collapsed into a single DataFrame table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def farthest_north(state_df):\n",
    "    # descending sort, then select row 0\n",
    "    # the datatype will be a pandas Series\n",
    "    return state_df.sort_values('INTPTLAT', ascending=False).iloc[0]\n",
    "\n",
    "df_joined.groupby('USPS').apply(farthest_north)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sorting by indices and columns\n",
    "\n",
    "We can sort by the row (or column) index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_by_state.sort_index(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also sort by the value in a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_by_state.sort_values('AMT_FHA').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Pandas HTML data import example\n",
    "\n",
    "Pandas takes a \"batteries included\" approach and throws in a whole lot of convenience functions.  For instance it has import functions for a variety of formats.  One of the pleasant surprises is a command `read_html` that's meant to automate the process of extracting tabular data from HTML.  In particular, it works pretty well with tables on Wikipedia.  \n",
    "\n",
    "Let's do an example: We'll try to extract the list of the world's tallest structures from\n",
    "http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfs = pd.read_html('http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world', header=0, parse_dates=False)\n",
    "\n",
    "# There are several tables on the page.  By inspection we can figure out which one we want\n",
    "tallest = dfs[2]\n",
    "\n",
    "# The coordinates column needs to be fixed up.  This is a bit of string parsing:\n",
    "def clean_lat_long(s):\n",
    "    try:\n",
    "        parts = s.split(\"/\")\n",
    "    except AttributeError:\n",
    "        return (None, None)\n",
    "    if len(parts) < 3:\n",
    "        return (None, None)\n",
    "    m = re.search(r\"(\\d+[.]\\d+);[^\\d]*(\\d+[.]\\d+)[^\\d]\", parts[2])\n",
    "    if not m:\n",
    "        return (None, None)\n",
    "    return (m.group(1), m.group(2))\n",
    "\n",
    "tallest['Clean_Coordinates'] = tallest['Coordinates'].apply(clean_lat_long)\n",
    "tallest['Latitude'] = tallest['Clean_Coordinates'].apply(lambda x:x[0])\n",
    "tallest['Longitude'] = tallest['Clean_Coordinates'].apply(lambda x:x[1])\n",
    "\n",
    "# Et voila\n",
    "tallest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "1. Parse the table rankings of [UK universities available on Wikipedia](https://en.wikipedia.org/wiki/Rankings_of_universities_in_the_United_Kingdom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pandas Timestamps\n",
    "\n",
    "Pandas comes with excellent tools for managing temporal data. Central to this is the Timestamp class, which can infer timestamps from many sensible inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print pd.Timestamp('July 4, 2016')\n",
    "print pd.Timestamp('Monday, July 4, 2016')\n",
    "print pd.Timestamp('Tuesday, July 4th, 2016')  # notice it ignored 'Tuesday'\n",
    "print pd.Timestamp('Monday, July 4th, 2016 05:00 PM')\n",
    "print pd.Timestamp('07/04/2016T17:20:13.123456')\n",
    "print pd.Timestamp(1467651600000000000)  # number of ns since the epoch, 1/1/1970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It can manage timestamps with timezones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "july4 = pd.Timestamp('Monday, July 4th, 2016 05:00 PM').tz_localize('US/Eastern')\n",
    "labor_day = pd.Timestamp('9/5/2016 12:00', tz='US/Eastern')\n",
    "thanksgiving = pd.Timestamp('11/24/2016 16:00')  # no timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Pandas can do calculations on Timestamps if they are both localized to the same timezone or neither has a timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print labor_day - july4\n",
    "# print thanksgiving - july4  # generates an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The time series offsets are useful for calculating dates relative to another date. Observe it skips over weekend days but is oblivious to holidays. Pandas does support [Custom Calendars](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#holidays-holiday-calendars) if you need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import BDay, Day, BMonthEnd\n",
    "\n",
    "print july4 + Day(5)  # 5 calendar days later, a Saturday.\n",
    "print july4 + BDay(5)  # 5 business days later, or the following Monday.\n",
    "print july4 - BDay(1)  # 1 business day earlier, or the previous Friday.\n",
    "print july4 + BMonthEnd(1)  # last business day of the month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Pandas can generate a range of dates. Here, we generate a list of business days in January of 2016:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "business_days = pd.date_range('1/1/2016', '1/31/2016', freq='B')\n",
    "business_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This can in turn be used in as a DataFrame index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_df = DataFrame(np.random.rand(len(business_days)),\n",
    "                    index=business_days,\n",
    "                   columns=['random'])\n",
    "time_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The same timezone functions are still available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_df.tz_localize('UTC').tz_convert('US/Pacific').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## DataFrame Resampling\n",
    "\n",
    "Pandas DataFrames provide a resampling method for changing the frequency of your data. Perhaps you have daily data but only care about the last data-point in each month. You can resample to downgrade the the data to a different frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "year2016 = pd.date_range(start='1/1/2016', end='12/31/2016')\n",
    "\n",
    "daily_data_df = DataFrame(np.random.rand(len(year2016)), index=year2016, columns=['random'])\n",
    "daily_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This makes a new DataFrame with each data-point coming from the last day of the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "monthly_data = daily_data_df.resample('1M').last()\n",
    "monthly_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What about resampling in the other direction? This is possible, but you may have gaps in the result. To avoid this, use the `ffill` and `bfill` methods, like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "daily_data2 = monthly_data.resample('1D').bfill()\n",
    "daily_data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You'll also notice that the first date in that DataFrame is 2016-01-31, not 2016-01-01 like in the original list. We can fix that if we want to. The `loc[year2016]` code fixes the index and the `bfill()` fills in the January values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "daily_data3 = daily_data2.loc[year2016].bfill()\n",
    "daily_data3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## A little bit more on `read_csv`\n",
    "\n",
    "We've already used the Pandas `read_csv` method a few times in this notebook, but let's take a closer look at what it can do. It is a powerful tool for inputting a wide variety of delimited text files and overcoming many typical data input problems. Because CSV files are so popular, over time many features were added and now it can handle pretty much any CSV file you are likely to find.\n",
    "\n",
    "You can access the `read_csv` documentation to read about all of the parameters, but here are the most useful ones.\n",
    "\n",
    "* sep: Change the file delimiter. By default it is a comma, but you will find files that use tabs ('\\t') and pipes. ('|')\n",
    "* header: Row number to use for column header. Some csv files have no header; in this case, pass None instead and use the names parameter.\n",
    "* names: List of column names to use, overriding what `read_csv` might name the columns from reading the file.\n",
    "* index_col: Make an index out of a column. Without this, it will create a default index of integers.\n",
    "* parse_dates: parse certain columns as dates.\n",
    "* infer_datetime_format: if the data in a column of dates all have the same format, this will speed up reading the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## DataFrames and Iterators\n",
    "\n",
    "Often you will need to loop through data in a DataFrame. Rather than use a `for` loop, you can convert a DataFrame into an iterator of your choosing.\n",
    "\n",
    "The `iterrows` method creates an iterator that returns tuples of the index value and its row of data as a Pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "list(daily_data_df.iterrows())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The `iteritems` method is similar except it iterates over columns, not rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "list(daily_data_df.iteritems())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, `itertuples`. This method returns each row as a tuple. Including the index value in that tuple is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "list(daily_data_df.itertuples(index=True, name=None))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercises\n",
    "\n",
    "1. Find a CSV file somewhere on the Internet that contains data that interests you. Open the file with the Pandas `read_csv` function and plot the data.\n",
    "1. Aggregate the data in a meaningful way and plot the results.\n",
    "1. Create a Pandas Timestamp for the day you were born and for today's date. How many days old are you? Seconds? Milliseconds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exit Tickets\n",
    "\n",
    "1. What is a Pandas DataFrame?\n",
    "1. What is a DataFrame index? How does it differ from a regular DataFrame column?\n",
    "1. What is DataFrame aggregation? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
